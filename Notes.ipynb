{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pprint import pprint\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear equation is $0=ax+by+c$. This provides a straight line through a 2D plane. \n",
    "\n",
    "When talking about machine learning, this is looked at as a matrix where:\n",
    "- $(x,y) \\rightarrow (x_1, x_2) = \\textbf{X}$\n",
    "- constants are renamed to $w_i$\n",
    "- the bias term / intercept $= w_0$\n",
    "\n",
    "$h(\\textbf{X}) = w_0 + w_1x_1 + w_2x_2 + w_nx_n$\n",
    "\n",
    "Where $h()$ is a *linear combination* of the components of $\\textbf{X}$. In vector form this is represented as $h(\\textbf{X}) = \\textbf{W}^T\\textbf{X}$ or $h(\\textbf{X}) = \\textbf{W}\\bullet\\textbf{X}$\n",
    "\n",
    "This allows us to work with planes (3D) or hyperplanes (>3D).\n",
    "\n",
    "To calculate the logistic regression, we are using a model that gives us the probability by passing it through a sigmoid function. There are various sigmoid functions but we will use the following:\n",
    "\n",
    "$\\sigma = \\frac{1}{1+ e^{-z}} \\in (0,1)$ with a $y$ intercept $=0$\n",
    "\n",
    "So to calculate our probability of an output belonging to a class, we will use:\n",
    "\n",
    "$y = \\sigma(\\textbf{W}^T\\textbf{X})$\n",
    "\n",
    "Logistic regression makes the assumption that the data can be separated by a line or a plane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the output of a logistic classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create normally distributed matrix\n",
    "N = 100\n",
    "D = 2\n",
    "\n",
    "X = np.random.randn(N,D)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a bias term = column of ones\n",
    "ones = np.array([[1]*N]).T\n",
    "ones.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate vectors to add the bias term\n",
    "Xb = np.concatenate((ones,X), axis=1)\n",
    "Xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly initialise a weight vector\n",
    "w = np.random.randn(D + 1)\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the dot product of X and W matrices\n",
    "z = Xb.dot(w)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.45953914, 0.49461177, 0.21312108, 0.53386388, 0.91529471,\n",
       "       0.64430297, 0.89616554, 0.8953878 , 0.53240738, 0.12782428,\n",
       "       0.32352807, 0.28921246, 0.30620428, 0.79895575, 0.36417114,\n",
       "       0.11955983, 0.15422503, 0.54304219, 0.56050085, 0.4601314 ,\n",
       "       0.35405477, 0.63573829, 0.14284789, 0.63046775, 0.67807454,\n",
       "       0.44202639, 0.80620858, 0.88566566, 0.65055425, 0.38111102,\n",
       "       0.22537933, 0.95732103, 0.58246147, 0.30358739, 0.24885075,\n",
       "       0.90928501, 0.31118029, 0.13352612, 0.31873313, 0.59053022,\n",
       "       0.92289846, 0.31194472, 0.3419616 , 0.14869494, 0.90288809,\n",
       "       0.71714416, 0.3206792 , 0.84955526, 0.51186351, 0.40488691,\n",
       "       0.79302755, 0.71168917, 0.7044027 , 0.58254855, 0.31110882,\n",
       "       0.59342342, 0.68958463, 0.81913778, 0.1551673 , 0.83078454,\n",
       "       0.91808648, 0.59885064, 0.30710988, 0.46490165, 0.5973267 ,\n",
       "       0.31027135, 0.26122523, 0.80469842, 0.75381591, 0.5366081 ,\n",
       "       0.3853422 , 0.96117809, 0.7864576 , 0.73192863, 0.78855927,\n",
       "       0.51188108, 0.25928943, 0.37957766, 0.76976558, 0.6748854 ,\n",
       "       0.40175435, 0.21555412, 0.48797027, 0.28920028, 0.54006558,\n",
       "       0.90620795, 0.49890022, 0.31636948, 0.47376581, 0.61521215,\n",
       "       0.52086518, 0.6589493 , 0.21878368, 0.12622791, 0.40191693,\n",
       "       0.35159074, 0.34895073, 0.34518229, 0.18622884, 0.521789  ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "sig_z = sigmoid(z) # Calculate probability of y given x\n",
    "sig_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of logistic regression is sigmoid that represents the probability that y belongs to class 1, given X\n",
    "\n",
    "$$output = p(y=1|x) = \\sigma(w^{T}x) \\in (0,1)$$\n",
    "\n",
    "$y$ can only take 2 values so:\n",
    "\n",
    "$$p(y=1|x) + p(y=0|x) = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ML we can then round the value so that if $x>0.5$ then predict one class, otherwise, predict the other.\n",
    "\n",
    "As the values move away from the dividing line, the value of $|w^{T}x|$ will tend to infinity, creating a clear definition on which group the point belongs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input features $x$ are weighted by $w$ through $\\textbf{W}^T\\textbf{X}$\n",
    "- Reformatted into a range (0,1) by the sigmoid through $y=\\sigma(\\textbf{W}^T\\textbf{X})$\n",
    "- $\\textbf{W}$ needs to be adjusted to provide the right answer > training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding parameters and weights so that the event being modeled is modelled correctly by the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem can be solved through a closed form solution. In general, problems must be solved through gradient descent but if the data is distributed in a specific way, they could be solved by closed form solution.\n",
    "\n",
    "Problem: <br>\n",
    "- Data from 2 Gaussian distributed classes\n",
    "- Same covariance but different means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayes rule\n",
    "\n",
    "$p(Y|X) = \\frac{p(X|Y)p(Y)}{p(X)}$\n",
    "\n",
    "- $p(X|Y)$ is the Gaussian - calculated by getting the mean and covariances of the data\n",
    "- Prior ($p(Y)$) is the maximum likelihood estimate (frequency estimate) $\\rightarrow$ e.g. $p(Y=1) = $ # times class 1 appeared / # total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We manipulate Bayes rule to fit in the logistic regression framework\n",
    "\n",
    "$P(y=1|x) = \\frac{p(x|y=1)(p(y=1)}{p(x)} = \\frac{p(x|y=1)(p(y=1)}{p(x|y=1)p(y=1)+p(x|y=0)p(y=0)}$\n",
    "\n",
    "$p(x) = p(x|y=1)p(y=1)+p(x|y=0)p(y=0)$ because $p(x)$ is the marginal of $p(x, y)$ (sum over all possible values of y - [resources](https://towardsdatascience.com/probability-concepts-explained-marginalisation-2296846344fc)) and $p(x, y) = p(x | y)p(y)$ as per the rules of conditional probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplify the equation by dividing by $p(x|y=1)p(y=1)$ and we get $P(y=1|x) = \\frac{1}{1+\\frac{p(x|y=0)p(y=0)}{p(x|y=1)(p(y=1)}}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
